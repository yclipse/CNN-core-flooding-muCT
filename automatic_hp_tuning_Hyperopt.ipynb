{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "164khMCRaR9PmGmhuOp2Plz2yTlqf1dSI",
      "authorship_tag": "ABX9TyNLZIoUOqwZ0HKUvmJbhyAI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yclipse/CNN-core-flooding-muCT/blob/master/automatic_hp_tuning_Hyperopt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNz4ubuaYgUh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install keras_unet_collection\n",
        "!pip install hyperas\n",
        "!pip install git+https://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@github.com/rtsmapping-yili/rtsmapping.git\n",
        "!pip install rioxarray\n",
        "!pip install hyperopt==0.2.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data():\n",
        "  import os\n",
        "  import numpy as np\n",
        "  import glob\n",
        "  from prep import dataio, utils\n",
        "  from skimage.transform import resize\n",
        "  import rasterio\n",
        "  import numpy as np\n",
        "  from prep import dataio, utils\n",
        "  from keras_unet_collection import models\n",
        "  def parsetif(fname):\n",
        "    with rasterio.open(fname) as src:\n",
        "      profile = src.profile\n",
        "      raw_tif = src.read()\n",
        "\n",
        "    raw_npy = raw_tif.copy()\n",
        "    removenan = np.nan_to_num(raw_npy,nan=0)\n",
        "    swapaxes = np.transpose(removenan, (1, 2, 0))\n",
        "    raw_array = swapaxes\n",
        "    for i in range(raw_array.shape[2]):\n",
        "      raw_array[...,i] = utils.normalise(raw_array[...,i])\n",
        "    raw_dim = (raw_array.shape[1],raw_array.shape[2])\n",
        "    return raw_array, raw_dim, profile\n",
        "\n",
        "  main_dir='/content/drive/MyDrive/data/DATA_V3_GEOTIFFS'\n",
        "  basemap='MAXAR'\n",
        "  train_list = glob.glob(os.path.join(main_dir,basemap,'train*','*'))\n",
        "  valtest_list = glob.glob(os.path.join(main_dir,basemap,'valtest*','*'))\n",
        "  _,val_list,test_list = dataio.split(valtest_list,train_split=0,val_split=0.5,test_split=0.5)\n",
        "\n",
        "  print ('total train images:',len(train_list))\n",
        "  print ('total val/test images:',len(val_list),len(test_list))\n",
        "\n",
        "  x_train = np.empty((len(train_list),192,192,8))\n",
        "  y_train = np.empty((len(train_list),192,192,2))\n",
        "  for ind,data in enumerate(train_list):\n",
        "    data_array,_,_ = parsetif(data)\n",
        "    im = data_array[...,:-1]\n",
        "    lb = data_array[...,-1]\n",
        "    lb[lb>0] = 1\n",
        "    im = resize(im, (192, 192), anti_aliasing=True)\n",
        "    lb = resize(lb, (192, 192), anti_aliasing=True)\n",
        "\n",
        "    x_train[ind] = im\n",
        "    y_train[ind] = utils.oneHotLabels(lb)\n",
        "\n",
        "  x_val = np.empty((len(val_list),192,192,8))\n",
        "  y_val = np.empty((len(val_list),192,192,2))\n",
        "  for ind,data in enumerate(val_list):\n",
        "    data_array,_,_ = parsetif(data)\n",
        "    im = data_array[...,:-1]\n",
        "    lb = data_array[...,-1]\n",
        "    lb[lb>0] = 1\n",
        "    im = resize(im, (192, 192), anti_aliasing=True)\n",
        "    lb = resize(lb, (192, 192), anti_aliasing=True)\n",
        "\n",
        "    x_val[ind] = im\n",
        "    y_val[ind] = utils.oneHotLabels(lb)\n",
        "\n",
        "  x_test = np.empty((len(test_list),192,192,8))\n",
        "  y_test = np.empty((len(test_list),192,192,2))\n",
        "  for ind,data in enumerate(test_list):\n",
        "    data_array,_,_ = parsetif(data)\n",
        "    im = data_array[...,:-1]\n",
        "    lb = data_array[...,-1]\n",
        "    lb[lb>0] = 1\n",
        "    im = resize(im, (192, 192), anti_aliasing=True)\n",
        "    lb = resize(lb, (192, 192), anti_aliasing=True)\n",
        "\n",
        "    x_test[ind] = im\n",
        "    y_test[ind] = utils.oneHotLabels(lb)\n",
        "  return x_train,y_train,x_val,y_val,x_test,y_test"
      ],
      "metadata": {
        "id": "R3R6jgnSYyIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.tools.docs.doc_controls import for_subclass_implementers\n",
        "from keras.utils.dataset_utils import labels_to_dataset\n",
        "from keras.backend import learning_phase\n",
        "from traitlets.config.application import T\n",
        "from networkx.classes.function import freeze\n",
        "from keras.utils.tf_inspect import stack\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperopt import hp\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def create_model():\n",
        "\n",
        "\n",
        "  filter_num = {{choice([[16, 32, 64, 128],\n",
        "              [32, 64, 128, 256],\n",
        "              [64, 128, 256, 512],\n",
        "              [16, 32, 64, 128, 256],\n",
        "              [32, 64, 128, 256, 512],\n",
        "              ])}}\n",
        "  activ = {{choice(['ReLU', 'GELU', 'Snake'])}}\n",
        "  out_activ = {{choice(['Sigmoid', 'Softmax'])}}\n",
        "  stack_num_up = {{choice([1,2])}}\n",
        "  stack_num_down = {{choice([1,2])}}\n",
        "  pooling = {{choice(['max', 'avg'])}} #\n",
        "  unpool = {{choice([False,True])}}\n",
        "  freeze_backbone = {{choice([False,True])}}\n",
        "  batch_norm = {{choice([False,True])}}\n",
        "  freeze_bn = {{choice([False,True])}}\n",
        "  train_size = (192,192)\n",
        "  batch_size = {{choice([4,8,16])}}\n",
        "  learning_rate = {{choice([1e-5,1e-4,1e-3,1e-2,1e-1])}}\n",
        "  label_smoothing = {{uniform(0, 1)}}\n",
        "  focal_loss_alpha = {{uniform(0, 1)}}\n",
        "\n",
        "  PARAMS = {'data': 'maxar',\n",
        "    'model': {'model' : 'Unet3+',\n",
        "          'input_size' : (None,None,8),\n",
        "          'filter_num' : filter_num,\n",
        "          'backbone' : 'EfficientNetB7',\n",
        "          'activation' : activ,\n",
        "          'out_activ' : out_activ,\n",
        "          'pooling' : pooling,\n",
        "          'batch_norm' : batch_norm,\n",
        "          'deep_supervision' : True,\n",
        "          'stack_num_up' : stack_num_up,\n",
        "          'stack_num_down' : stack_num_down,\n",
        "          'unpool' : unpool,\n",
        "          'freeze_backbone' : freeze_backbone,\n",
        "          'freeze_bn' : freeze_bn,\n",
        "          },\n",
        "    'train': {'train_size' : train_size,\n",
        "          'batch_size' : batch_size,\n",
        "          'learning_rate' : learning_rate,\n",
        "          'lr_reduce_tol' : 10,\n",
        "          'epochs' : 300,\n",
        "          'early_stopping' : 30,\n",
        "          'label_smoothing' : label_smoothing,\n",
        "          'adam_epsilon' : 1e-9,\n",
        "          'amsgrad' : True,\n",
        "          'focal_loss_alpha' : focal_loss_alpha,\n",
        "          },\n",
        "\n",
        "    'model_name' : \"MODEL_V2_UNET3+_MAXAR_192x192\",\n",
        "\n",
        "    'Hyperopt' :'no',\n",
        "\n",
        "    }\n",
        "  model = models.unet_3plus_2d(input_size=PARAMS['model']['input_size'],\n",
        "              n_labels=2,\n",
        "              filter_num_down=PARAMS['model']['filter_num'],\n",
        "              filter_num_skip='auto',\n",
        "              filter_num_aggregate='auto',\n",
        "              stack_num_down=PARAMS['model']['stack_num_down'],\n",
        "              stack_num_up=PARAMS['model']['stack_num_up'],\n",
        "              activation=PARAMS['model']['activation'],\n",
        "              output_activation=PARAMS['model']['out_activ'],\n",
        "              batch_norm=PARAMS['model']['batch_norm'],\n",
        "              pool=PARAMS['model']['pooling'],\n",
        "              unpool=PARAMS['model']['unpool'],\n",
        "              deep_supervision=PARAMS['model']['deep_supervision'],\n",
        "              backbone=PARAMS['model']['backbone'],\n",
        "              weights=None,\n",
        "              freeze_backbone=PARAMS['model']['freeze_backbone'],\n",
        "              freeze_batch_norm=PARAMS['model']['freeze_bn'],\n",
        "              name='unet3plus')\n",
        "\n",
        "  adam = tf.keras.optimizers.Adam(learning_rate=PARAMS['train']['learning_rate'],\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999,\n",
        "                epsilon=PARAMS['train']['adam_epsilon'],\n",
        "                amsgrad=PARAMS['train']['amsgrad'],\n",
        "                name='Adam')\n",
        "\n",
        "  bfc = tf.keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True,\n",
        "                          alpha=PARAMS['train']['focal_loss_alpha'],\n",
        "                          gamma=2.0,\n",
        "                          from_logits=False,\n",
        "                          label_smoothing=PARAMS['train']['label_smoothing'],\n",
        "                          axis=-1,\n",
        "                          reduction=tf.keras.losses.Reduction.AUTO,\n",
        "                          name='binary_focal_crossentropy'\n",
        "                          )\n",
        "\n",
        "\n",
        "  iou = [tf.keras.metrics.OneHotMeanIoU(num_classes=2, name='iou')]\n",
        "\n",
        "  model.compile(loss=bfc,\n",
        "        optimizer=adam,\n",
        "        metrics=iou,\n",
        "        )\n",
        "\n",
        "\n",
        "  print('start training')\n",
        "  result = model.fit(x_train,\n",
        "        y_train,\n",
        "        batch_size = PARAMS['train']['batch_size'],\n",
        "        validation_data = (x_val, y_val),\n",
        "        epochs=50,\n",
        "        )\n",
        "\n",
        "  score, acc = model.evaluate(x_test, y_test)\n",
        "  print('Test accuracy:', acc)\n",
        "\n",
        "  return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rAmSC60eegqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#enable gpu\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dgzi1G1lVgJ",
        "outputId": "2dbbb92d-f69a-4f32-db89-ada3be9f5670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/Colab_Notebooks/unet3+/Model_V2.1_geotiff_data')\n",
        "# x_train,y_train,x_val,y_val,x_test,y_test = data()"
      ],
      "metadata": {
        "id": "AqxgTWMHlV4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R5W82PnywgbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  best_run, best_model = optim.minimize(model=create_model,\n",
        "                    data = data,\n",
        "                    algo=tpe.suggest,\n",
        "                    max_evals=10,\n",
        "                    trials=Trials(),\n",
        "                    notebook_name='Hyperas_1June')\n",
        "\n",
        "  # print(\"Evalutation of best performing model:\")\n",
        "  # print(best_model.evaluate(x_test, y_test))\n",
        "  print(\"Best performing model chosen hyper-parameters:\")\n",
        "  print(best_run)"
      ],
      "metadata": {
        "id": "TEU9VTM2noWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Arz-JpCOw38u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "best = pd.DataFrame(best_run, index = [0])\n",
        "print(best)\n",
        "# best_model"
      ],
      "metadata": {
        "id": "-qsFvdJqlXvz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}